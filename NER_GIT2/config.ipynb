{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DefaultConfig():\n",
    "    ###     Path     ###\n",
    "    #   graph Path\n",
    "    output_path = \"results/\"\n",
    "    model_output = output_path + \"model.weights/\"\n",
    "    log_path = output_path + \"logs/\"\n",
    "    # preprocessed Data path\n",
    "    test_filename = \"data/test_data\"\n",
    "    train_filename = \"data/train_data\"\n",
    "    #word2vec_filename = \"data/wikipedia-200-mincount-20-window-8-cbow.bin\"\n",
    "    word2vec_filename = \"data/wikipedia-100-mincount-30-window-8-cbow.bin\"\n",
    "    # preprocessed Data file names\n",
    "    tags_filename = \"data/tags.txt\"\n",
    "    words_filename = \"data/words.txt\"\n",
    "    chars_filename = \"data/chars.txt\"\n",
    "    # preprocessed data variables\n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"O\"\n",
    "\n",
    "    ###  Hyper parameters    ###\n",
    "    BATCH_SIZE = 40\n",
    "    MAX_LENGTH_WORD = 50\n",
    "    N_EPOCHS = 50\n",
    "    LR = 0.001\n",
    "    LR_DECAY = 0.95\n",
    "    DROPOUT = 0.5\n",
    "    # Char Embedding (CNN)\n",
    "    CHAR_EMB_DIM = 120\n",
    "    FILTER_SIZE = [2, 3, 4, 5]\n",
    "    N_FILTERS = 128\n",
    "    # BiLSTM\n",
    "    HIDDEN_SIZE = 400\n",
    "\n",
    "    ### Tensorflow op   ###\n",
    "    NUM_THREADS = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "#from config import DefaultConfig as cfg\n",
    "\n",
    "class conllReader(object):\n",
    "    \"\"\"\n",
    "    This class will iterate over CoNLL dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, processing_word=None, processing_tag=None):\n",
    "\n",
    "        self.filename = filename\n",
    "        self.processing_word = processing_word\n",
    "        self.processing_tag = processing_tag\n",
    "        self.length = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, encoding='utf-8') as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    if len(words) != 0:\n",
    "                        yield words, tags\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split('\\t')\n",
    "                    word, tag = ls[1], ls[5]\n",
    "                    if self.processing_word is not None:\n",
    "                        word = self.processing_word(word)\n",
    "                    if self.processing_tag is not None:\n",
    "                        tag = self.processing_tag(tag)\n",
    "                    words += [word]\n",
    "                    tags += [tag]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Iterates once over the corpus to set and store length\n",
    "        \"\"\"\n",
    "        if self.length is None:\n",
    "            self.length = 0\n",
    "            for _ in self:\n",
    "                self.length += 1\n",
    "\n",
    "        return self.length\n",
    "\n",
    "\n",
    "def get_vocabs(datasets):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        datasets: a list of dataset objects\n",
    "    Return:\n",
    "        a set of all the words in the dataset\n",
    "    \"\"\"\n",
    "    print(\"Building vocab...\")\n",
    "    vocab_words = set()\n",
    "    vocab_tags = set()\n",
    "    for dataset in datasets:\n",
    "        for words, tags in dataset:\n",
    "            vocab_words.update(words)\n",
    "            vocab_tags.update(tags)\n",
    "    print(\"- done. {} tokens\".format(len(vocab_words)))\n",
    "    return vocab_words, vocab_tags\n",
    "\n",
    "\n",
    "def get_char_vocab(dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset: a iterator yielding tuples (sentence, tags)\n",
    "    Returns:\n",
    "        a set of all the characters in the dataset\n",
    "    \"\"\"\n",
    "    vocab_char = set()\n",
    "    for words, _ in dataset:\n",
    "        for word in words:\n",
    "            vocab_char.update(word)\n",
    "\n",
    "    return vocab_char\n",
    "\n",
    "\n",
    "def get_sentences(datasets):\n",
    "    \"\"\"\n",
    "    :param dataset: an iterator yielding tuples (sentence, tags)\n",
    "    :return: a list of sentences\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for dataset in datasets:\n",
    "        for sentence, _ in dataset:\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def write_vocab(vocab, filename):\n",
    "    \"\"\"\n",
    "    Writes a vocab to a file\n",
    "    Args:\n",
    "        vocab: iterable that yields word\n",
    "        filename: path to vocab file\n",
    "    Returns:\n",
    "        write a word per line\n",
    "    \"\"\"\n",
    "    print(\"Writing vocab...\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for i, word in enumerate(vocab):\n",
    "            if i != len(vocab) - 1:\n",
    "                f.write(\"{}\\n\".format(word))\n",
    "            else:\n",
    "                f.write(word)\n",
    "    print(\"- done. {} tokens\".format(len(vocab)))\n",
    "\n",
    "def load_vocab(filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filename: file with a word per line\n",
    "    Returns:\n",
    "        d: dict[word] = index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = dict()\n",
    "        with open(filename) as f:\n",
    "            for idx, word in enumerate(f):\n",
    "                word = word.strip()\n",
    "                d[word] = idx\n",
    "\n",
    "    except IOError:\n",
    "        print(\"Error loading file\")\n",
    "    return d\n",
    "\n",
    "def get_processing_word(vocab_words=None, vocab_chars=None,\n",
    "                    lowercase=False, chars=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: dict[word] = idx\n",
    "    Returns:\n",
    "        f(\"cat\") = ([12, 4, 32], 12345)\n",
    "                 = (list of char ids, word id)\n",
    "    \"\"\"\n",
    "    def f(word):\n",
    "        # 0. get chars of words\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            char_ids = []\n",
    "            for char in word:\n",
    "                # ignore chars out of vocabulary\n",
    "                if char in vocab_chars:\n",
    "                    char_ids += [vocab_chars[char]]\n",
    "\n",
    "        # 1. preprocess word\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = cfg.NUM\n",
    "\n",
    "        # 2. get id of word\n",
    "        if vocab_words is not None:\n",
    "            if word in vocab_words:\n",
    "                word = vocab_words[word]\n",
    "            else:\n",
    "                word = vocab_words[cfg.UNK]\n",
    "\n",
    "        # 3. return tuple char ids, word id\n",
    "        if vocab_chars is not None and chars == True:\n",
    "            return char_ids, word\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from config import DefaultConfig as cfg\n",
    "\n",
    "def pad_sequences(sequences, pad_token, type):\n",
    "    '''\n",
    "    add pad_token to the words, or sentences to have same length\n",
    "    :param sequences: a list of words or sentences\n",
    "    :param pad_token: the value should be added to all sequences\n",
    "    :param type: either 'words' or 'sentences'\n",
    "    :return: a list of words or sentences with same length\n",
    "    '''\n",
    "    if type == 'sentences':\n",
    "        max_length = max(map(lambda x: len(x), sequences))\n",
    "        sequence_padded, sequence_length = add_pad(sequences, pad_token, max_length)\n",
    "\n",
    "    elif type == 'words':\n",
    "        max_length_word = cfg.MAX_LENGTH_WORD#max([max(map(lambda x : len(x), seq)) for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            sp, sl = add_pad(seq, pad_token, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x: len(x), sequences))\n",
    "        sequence_padded, _ = add_pad(sequence_padded, [pad_token]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = add_pad(sequence_length, 0, max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def add_pad(sequences, pad_token, max_length):\n",
    "    '''\n",
    "    add pad to sequences\n",
    "    :param sequences: a list\n",
    "    :param pad_token: pad token\n",
    "    :param max_length: maximum length to be padded\n",
    "    :return: Padded sequence and sequence length\n",
    "    '''\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_token]*max(max_length - len(seq), 0)\n",
    "        sequence_padded += [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def batch_gen(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "    Returns:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "    \"\"\"\n",
    "    default = tags[cfg.NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from config import DefaultConfig as cfg\n",
    "\n",
    "def pad_sequences(sequences, pad_token, type):\n",
    "    '''\n",
    "    add pad_token to the words, or sentences to have same length\n",
    "    :param sequences: a list of words or sentences\n",
    "    :param pad_token: the value should be added to all sequences\n",
    "    :param type: either 'words' or 'sentences'\n",
    "    :return: a list of words or sentences with same length\n",
    "    '''\n",
    "    if type == 'sentences':\n",
    "        max_length = max(map(lambda x: len(x), sequences))\n",
    "        sequence_padded, sequence_length = add_pad(sequences, pad_token, max_length)\n",
    "\n",
    "    elif type == 'words':\n",
    "        max_length_word = cfg.MAX_LENGTH_WORD#max([max(map(lambda x : len(x), seq)) for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            sp, sl = add_pad(seq, pad_token, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x: len(x), sequences))\n",
    "        sequence_padded, _ = add_pad(sequence_padded, [pad_token]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = add_pad(sequence_length, 0, max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def add_pad(sequences, pad_token, max_length):\n",
    "    '''\n",
    "    add pad to sequences\n",
    "    :param sequences: a list\n",
    "    :param pad_token: pad token\n",
    "    :param max_length: maximum length to be padded\n",
    "    :return: Padded sequence and sequence length\n",
    "    '''\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_token]*max(max_length - len(seq), 0)\n",
    "        sequence_padded += [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length\n",
    "\n",
    "\n",
    "def batch_gen(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "    Returns:\n",
    "        list of tuples\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "    \"\"\"\n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "    \"\"\"\n",
    "    default = tags[cfg.NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from data_helper import pad_sequences, batch_gen, get_chunks\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, config, embeddings, ntags, nchars):\n",
    "        '''\n",
    "        Tensorflow model\n",
    "        :param embeddings: word2vec embedding file which loaded\n",
    "        :param ntags: number of tags\n",
    "        :param nchars: number of chars\n",
    "        '''\n",
    "        self.cfg = config\n",
    "        self.embeddings = embeddings\n",
    "        self.nchars = nchars\n",
    "        self.ntags = ntags\n",
    "\n",
    "        self.add_placeholders()                 # Initial placeholders\n",
    "        self.add_word_embeddings_op()           # add embedding operation to graph\n",
    "        self.add_logits_op()                    # add logits operation to graph\n",
    "        self.add_loss_op()                      # add loss operation to graph\n",
    "        self.add_train_op()                     # add train (optimzier) operation to graph\n",
    "\n",
    "        # Merge all summaries into a single op\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        '''\n",
    "        Initial placeholders\n",
    "        '''\n",
    "        # Shape = (batch size, max length of sentences in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name=\"word_ids\")\n",
    "\n",
    "        # Shape = (batch size)\n",
    "        self.sentences_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sentences_lengths\")\n",
    "\n",
    "        # Shape = (batch size, max length of sentences, max length of words)\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name=\"char_ids\")\n",
    "\n",
    "        # Shape = (batch size, max length of sentences)\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_length\")\n",
    "\n",
    "        # Shape = (batch size, max length of sentences)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\")\n",
    "\n",
    "        # Learning rate for Optimization\n",
    "        self.lr = tf.placeholder(tf.float32, shape=[], name=\"Learning_rate\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = tf.placeholder(tf.float32, shape=[], name=\"Dropout\")\n",
    "\n",
    "\n",
    "    def add_word_embeddings_op(self):\n",
    "        '''\n",
    "        Add word embedings + Char CNN operation to graph\n",
    "        '''\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            _word_embeddings = tf.Variable(self.embeddings, name=\"_word_embeddings\", dtype=tf.float32, trainable=False)\n",
    "            word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=\"word_embeddings\")\n",
    "\n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            xavi = tf.contrib.layers.xavier_initializer\n",
    "            # Get char level embeddings matrix\n",
    "            _char_embeddings = tf.get_variable(\"_char_embeddings\", shape=[self.nchars, self.cfg.CHAR_EMB_DIM],\n",
    "                                               dtype=tf.float32,\n",
    "                                               initializer=xavi())\n",
    "            self.char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n",
    "                                                     self.char_ids,\n",
    "                                                     name=\"char_embeddings\")\n",
    "            # get shape of char embd matrix\n",
    "            s = tf.shape(self.char_embeddings)\n",
    "            # Reshape char_embd matrix to [batches * sentence length , max_word_length , char_embedding_size]\n",
    "            self.char_embeddings = tf.reshape(self.char_embeddings, [-1, self.cfg.MAX_LENGTH_WORD, self.cfg.CHAR_EMB_DIM])\n",
    "            # Add one dimension at the end of char_emb matrix to have shape like:\n",
    "            # [batches, height, width, channels] like an image. Here channel=1\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.char_embeddings, -1)\n",
    "\n",
    "            # Create a convolution + maxpool layer for each filter size\n",
    "            pooled_outputs = []\n",
    "            # Here we do convolution over [words x char_emb] with different filter sizes [2,3,4,5].\n",
    "            for i, filter_size in enumerate(self.cfg.FILTER_SIZE):\n",
    "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                    # Define convolution filter Layer with shape = [filter_height, filter_width, in_channels, out_channels]\n",
    "                    filter_shape = [filter_size, self.cfg.CHAR_EMB_DIM, 1, self.cfg.N_FILTERS]\n",
    "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_char\")\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[self.cfg.N_FILTERS]), name=\"b_char\")\n",
    "                    # conv return shape= [batch * sentence_length, MAX_LENGTH_WORD - FILTER_SIZE + 1, 1, N_FILTERS]\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    # Apply nonlinearity\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\") # h has same shape as conv\n",
    "                    # Maxpooling over the outputs\n",
    "                    # return shape= [Batch_size, 1, 1, N_FILTERS]\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, self.cfg.MAX_LENGTH_WORD - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool\")\n",
    "                    # Add all convolution outputs to a list\n",
    "                    pooled_outputs.append(pooled)\n",
    "\n",
    "            # Combine all the pooled features\n",
    "            num_filters_total = self.cfg.N_FILTERS * len(self.cfg.FILTER_SIZE)\n",
    "            # Concatinate all pooled features over 3th dimension\n",
    "            self.h_pool = tf.concat(pooled_outputs, 3) # has shape= [Batch_size, 1, 1, len(FILTER_SIZE) * N_FILTERS]\n",
    "            # Reshape data to shape= [Batch_Size, Sentence_Length, len(FILTER_SIZE) * N_FILTERS]\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, s[1], num_filters_total])\n",
    "            # Add char features to embedding words. Shape= [Batch_Size, Sentence_Length, Word_Emb_length + len(FILTER_SIZE) * N_FILTERS]\n",
    "            word_embeddings = tf.concat([word_embeddings, self.h_pool_flat], axis=-1)\n",
    "        # add Dropout regularization\n",
    "        self.word_embeddings = tf.nn.dropout(word_embeddings, self.dropout)\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"\n",
    "        add pad to the data and build feed data for tensorflow\n",
    "        :param words: data\n",
    "        :param labels: labels\n",
    "        :param lr: learning rate\n",
    "        :param dropout: dropout probability\n",
    "        :return: padded data with their corresponding length\n",
    "        \"\"\"\n",
    "        # Unzip data to char_ids and word_ids\n",
    "        char_ids, word_ids = zip(*words)\n",
    "        # pad sentence to maximum sentence length of current batch\n",
    "        word_ids, sentences_lengths = pad_sequences(word_ids, 0, type='sentences')\n",
    "        # pad words to maximum word length of current batch\n",
    "        char_ids, word_lengths = pad_sequences(char_ids, pad_token=0, type='words')\n",
    "\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.sentences_lengths: sentences_lengths,\n",
    "            self.char_ids: char_ids,\n",
    "            self.word_lengths: word_lengths\n",
    "        }\n",
    "\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0, type='sentences')\n",
    "            feed[self.labels] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "\n",
    "        return feed, sentences_lengths\n",
    "\n",
    "\n",
    "    def add_logits_op(self):\n",
    "        \"\"\"\n",
    "        Adds logits to Model. We use BiLSTM + fully connected layer to predict word sequences labels\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            # Define Forwards cell\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.cfg.HIDDEN_SIZE)\n",
    "            # Define Backwards cell\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.cfg.HIDDEN_SIZE)\n",
    "            # Run BiLSTM\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "                                                                        cell_bw, self.word_embeddings,\n",
    "                                                                        sequence_length=self.sentences_lengths,\n",
    "                                                                        dtype=tf.float32)\n",
    "            # Concatenate Forward and backward over last axis\n",
    "            # The shape is: [Batch_size, Sentence_length, 2*HIDDEN_SIZE]\n",
    "            rnn_output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            # Apply Dropout regularization\n",
    "            rnn_output = tf.nn.dropout(rnn_output, self.dropout)\n",
    "\n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            # Define weights and Biases\n",
    "            W1 = tf.get_variable(\"W1\", shape=[2 * self.cfg.HIDDEN_SIZE, self.cfg.HIDDEN_SIZE],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            b1 = tf.get_variable(\"b1\", shape=[self.cfg.HIDDEN_SIZE], dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "\n",
    "            W2 = tf.get_variable(\"W2\", shape=[self.cfg.HIDDEN_SIZE, self.ntags],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            b2 = tf.get_variable(\"b2\", shape=[self.ntags], dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            # get sentence length\n",
    "            ntime_steps = tf.shape(rnn_output)[1]\n",
    "            # Reshape to 2D to calculate W1. shape= [Batch_size * sentences_length, 2*HIDDEN_SIZE]\n",
    "            rnn_output = tf.reshape(rnn_output, [-1, 2 * self.cfg.HIDDEN_SIZE])\n",
    "            # Apply projection, return [Batch_size * sentences_length, HIDDEN_SIZE]\n",
    "            w1_output = tf.matmul(rnn_output, W1) + b1\n",
    "            # Apply nonlinearity\n",
    "            w1_output = tf.nn.relu(w1_output, name=\"w1_relu\")\n",
    "            # Apply Dropout regularization\n",
    "            w1_output = tf.nn.dropout(w1_output, self.dropout)\n",
    "            # Apply projection, return shape= [Batch_size * sentences_length, N_Tags]\n",
    "            pred = tf.matmul(w1_output, W2) + b2\n",
    "            # Return back to shape= [[Batch_size , sentences_length, N_Tags]\n",
    "            self.logits = tf.reshape(pred, [-1, ntime_steps, self.ntags])\n",
    "\n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"\n",
    "        Adds loss to Model\n",
    "        \"\"\"\n",
    "        # Get highest probabilty of predicted labels\n",
    "        self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32)\n",
    "        # Compute loss\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "        # Use Mask to eliminate Zeros paddings\n",
    "        mask = tf.sequence_mask(self.sentences_lengths)\n",
    "        losses = tf.boolean_mask(losses, mask)\n",
    "        # assign loss to self\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # Create a summary to monitor loss\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "\n",
    "    def add_train_op(self):\n",
    "        \"\"\"\n",
    "        Add train_op to Model\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"train_step\"):\n",
    "            # In each epoch iteration, the Learning Rate will decay which defined in config file\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "    def predict_batch(self, sess, words, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: a tensorflow session\n",
    "            words: list of sentences\n",
    "            labels: list of true labels\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length: length of sentences\n",
    "            loss: loss of current batch\n",
    "        \"\"\"\n",
    "        # get the feed dictionnary\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, labels, dropout=1.0)\n",
    "        # Run Tensorflow graph\n",
    "        labels_pred, loss = sess.run([self.labels_pred, self.loss], feed_dict=fd)\n",
    "        return labels_pred, sequence_lengths, loss\n",
    "\n",
    "\n",
    "    def run_evaluate(self, sess, test, tags):\n",
    "        \"\"\"\n",
    "        Evaluates performance on dev set\n",
    "        Args:\n",
    "            sess: tensorflow session\n",
    "            test: dataset that yields tuple of sentences, tags\n",
    "            tags: {tag: index} dictionary\n",
    "        Returns:\n",
    "            accuracy\n",
    "            f1 score\n",
    "            loss\n",
    "            Precision\n",
    "            Recall\n",
    "        This code honored to:\n",
    "        https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html\n",
    "        \"\"\"\n",
    "        accs = []\n",
    "        losses = 0.0\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in batch_gen(test, self.cfg.BATCH_SIZE):\n",
    "            labels_pred, sequence_lengths, loss = self.predict_batch(sess, words, labels)\n",
    "            losses += loss\n",
    "            for lab, lab_pred, length in zip(labels, labels_pred, sequence_lengths):\n",
    "                lab = lab[:length] #TODO: it is useless!\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "                lab_chunks = set(get_chunks(lab, tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred, tags))\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)\n",
    "\n",
    "        p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "         # Create a summary to monitor accuracy\n",
    "        tf.summary.scalar(\"accuracy\", acc)\n",
    "        # Create a summary to monitor Precision\n",
    "        tf.summary.scalar(\"accuracy\", p)\n",
    "        # Create a summary to monitor Recall\n",
    "        tf.summary.scalar(\"accuracy\", r)\n",
    "        return acc, f1, losses, p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: C:\\LocalDiskD\\ProgrammingRequisites\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py wikipedia-xxx-mincount-xx-window-x-cbow.bin TRAIN_SET DEV_SET\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\LocalDiskD\\ProgrammingRequisites\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#import conll_reader as cr\n",
    "#from config import DefaultConfig\n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "#from model import Model\n",
    "#from data_helper import batch_gen\n",
    "\n",
    "\n",
    "def train_model(cfg, train_set, dev_set, embed, tags, chars):\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(cfg, embed, len(tags), len(chars))\n",
    "\n",
    "    # initial session\n",
    "    with tf.Session() as sess:\n",
    "    # with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=cfg.NUM_THREADS)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # create log writer\n",
    "        summary_writer = tf.summary.FileWriter(cfg.log_path, graph=tf.get_default_graph())\n",
    "        # run epoch\n",
    "        for epoch in range(cfg.N_EPOCHS):\n",
    "            train_losses = 0.0\n",
    "            validation_loss = 0.0\n",
    "            accuracy = 0.0\n",
    "            # Run batches\n",
    "            i = 0 # counter for summary results.\n",
    "            for words, labels in batch_gen(train_set, cfg.BATCH_SIZE):\n",
    "                fd, _ = model.get_feed_dict(words, labels, cfg.LR, cfg.DROPOUT)\n",
    "                # train model\n",
    "                _, train_loss, summary = sess.run([model.train_op, model.loss, model.merged_summary_op], feed_dict=fd)\n",
    "                train_losses += train_loss\n",
    "                # Write logs at every iteration\n",
    "                summary_writer.add_summary(summary, epoch * cfg.BATCH_SIZE + i)\n",
    "                i += 1\n",
    "            # Evaluate model after training\n",
    "            accuracy, f1, validation_loss, p, r = model.run_evaluate(sess, dev_set, tags)\n",
    "            # decay learning rate\n",
    "            cfg.LR *= cfg.LR_DECAY\n",
    "\n",
    "            print(\"epoch %d - train loss: %.2f, validation loss: %.2f, accuracy: %.2f  with f1: %.2f , P: %.2f , R: %.2f \" % \\\n",
    "                (epoch + 1, train_losses, validation_loss, accuracy * 100, f1 * 100, p * 100, r * 100))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 4:\n",
    "        sys.stderr.write(\"Usage: %s wikipedia-xxx-mincount-xx-window-x-cbow.bin TRAIN_SET DEV_SET\\n\" % sys.argv[0])\n",
    "        sys.exit(1)\n",
    "\n",
    "    cfg = DefaultConfig()\n",
    "\n",
    "    # check if data not processed, generate tags, words, chars\n",
    "    if not (os.path.exists(cfg.words_filename) & os.path.exists(cfg.tags_filename) & os.path.exists(cfg.chars_filename)):\n",
    "        print(\"preprocessed Data not found. processing data...\")\n",
    "        train = cr.conllReader(sys.argv[2])\n",
    "        test = cr.conllReader(sys.argv[3])\n",
    "\n",
    "        # get words and tags vocabulary from whole data\n",
    "        vocab_words, vocab_tags = cr.get_vocabs([train, test])\n",
    "        # Add unknown token and number to vocab\n",
    "        vocab_words.add(cfg.UNK)\n",
    "        vocab_words.add(cfg.NUM)\n",
    "        # save all words and tags to file\n",
    "        cr.write_vocab(vocab_tags, cfg.tags_filename)\n",
    "        cr.write_vocab(vocab_words, cfg.words_filename)\n",
    "        # get and save chars from dataset to file\n",
    "        vocab_chars = cr.get_char_vocab(train)\n",
    "        cr.write_vocab(vocab_chars, cfg.chars_filename)\n",
    "\n",
    "    # load preprocessed vocabs\n",
    "    try:\n",
    "        vocab_words = cr.load_vocab(cfg.words_filename)\n",
    "        vocab_tags  = cr.load_vocab(cfg.tags_filename)\n",
    "        vocab_chars = cr.load_vocab(cfg.chars_filename)\n",
    "    except IOError:\n",
    "        print(\"Error loading words, tags, chars files\")\n",
    "\n",
    "    # Load wikipedia-200-mincount-20-window-8-cbow embedding file\n",
    "    # Load wikipedia-xxx-mincount-xx-window-x-cbow embedding file\n",
    "    try:\n",
    "        word2vec = gensim.models.KeyedVectors.load_word2vec_format(sys.argv[1], binary=True)\n",
    "        embeddings = word2vec.syn0\n",
    "    except IOError:\n",
    "        print(\"error loading file with genism: wikipedia-200-mincount-20-window-8-cbow\")\n",
    "\n",
    "    # assign processing options to processing function\n",
    "    processing_word = cr.get_processing_word(vocab_words, vocab_chars,\n",
    "                    lowercase=False, chars=True)\n",
    "    processing_tag  = cr.get_processing_word(vocab_tags,\n",
    "                    lowercase=False)\n",
    "    # read trian and test set\n",
    "    train = cr.conllReader(sys.argv[2], processing_word, processing_tag)\n",
    "    test = cr.conllReader(sys.argv[3], processing_word, processing_tag)\n",
    "    # train and test model\n",
    "    train_model(cfg, train, test,  embeddings, vocab_tags, vocab_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
